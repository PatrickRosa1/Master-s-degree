{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEKwBhiDsWuq3JruEjmBS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickRosa1/Master-s-degree/blob/main/Modelo_Hardware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model to hardware"
      ],
      "metadata": {
        "id": "j3j6eYmK6vKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset e libraries"
      ],
      "metadata": {
        "id": "vLkCDHM07UjW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcvtMETd6qnv",
        "outputId": "8271b0d5-027a-4354-e908-9f09e3e44374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dzrnznAbTkTKGmcByriAZ9kg7dwncnR0\n",
            "From (redirected): https://drive.google.com/uc?id=1dzrnznAbTkTKGmcByriAZ9kg7dwncnR0&confirm=t&uuid=c4af78aa-b741-4873-84aa-86edc81bd8fd\n",
            "To: /content/sample_100pc.csv\n",
            " 37% 657M/1.79G [00:07<00:11, 95.9MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 754, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 879, in _raw_read\n",
            "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 862, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 473, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py\", line 172, in main\n",
            "    download(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gdown/download.py\", line 368, in download\n",
            "    for chunk in res.iter_content(chunk_size=CHUNK_SIZE):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 820, in generate\n",
            "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 1066, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 955, in read\n",
            "    data = self._raw_read(amt)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 878, in _raw_read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 800, in _error_catcher\n",
            "    self._connection.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 322, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 981, in close\n",
            "    sock.close()   # close it manually... there may be other refs\n",
            "    ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 503, in close\n",
            "    self._real_close()\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1373, in _real_close\n",
            "    super()._real_close()\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 497, in _real_close\n",
            "    _ss.close(self)\n",
            "KeyboardInterrupt\n",
            " 37% 667M/1.79G [00:07<00:12, 88.3MB/s]\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#dataset completo colunas invariaveis removidas\n",
        "!gdown 1dzrnznAbTkTKGmcByriAZ9kg7dwncnR0 #sample_100pc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "#from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.tree import plot_tree\n",
        "import gc\n",
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUXkUiXT7KNZ",
        "outputId": "da6b81ba-5ac0-417f-b2cf-c0ff108475dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Target"
      ],
      "metadata": {
        "id": "jblqlnSr7-LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"continuar\"\n",
        "targetx= \"filtro_final_x\"\n",
        "targety= \"filtro_final_y\"\n",
        "\n",
        "\n",
        "# Reg_Reg\n",
        "def calcular_continuarRR(row):\n",
        "    if row['filtro_final_x']==0 and row['filtro_final_y']==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Reg_Smo\n",
        "def calcular_continuarRM(row):\n",
        "    if row['filtro_final_x']==0 and row['filtro_final_y']==1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Reg_sha\n",
        "def calcular_continuarRH(row):\n",
        "    if row['filtro_final_x']==0 and row['filtro_final_y']==2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "# smo_reg\n",
        "def calcular_continuarMR(row):\n",
        "    if row['filtro_final_x']==1 and row['filtro_final_y']==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# smo_smo\n",
        "def calcular_continuarMM(row):\n",
        "    if row['filtro_final_x']==1 and row['filtro_final_y']==1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# smo_sha\n",
        "def calcular_continuarMH(row):\n",
        "    if row['filtro_final_x']==1 and row['filtro_final_y']==2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# sha_reg\n",
        "def calcular_continuarHR(row):\n",
        "    if row['filtro_final_x']==2 and row['filtro_final_y']==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# sha_smo\n",
        "def calcular_continuarHM(row):\n",
        "    if row['filtro_final_x']==2 and row['filtro_final_y']==1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# sha_sha\n",
        "def calcular_continuarHH(row):\n",
        "    if row['filtro_final_x']==2 and row['filtro_final_y']==2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Regula em x ou y\n",
        "def calcular_continuar_anyR(row):\n",
        "    if row['filtro_final_x'] and row['filtro_final_y']:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# Função para calcular a coluna \"continuar\"\n",
        "def calcular_continuar0(row):\n",
        "    filtro_testado = 0\n",
        "    filtro_final = row['filtro_final_x']\n",
        "\n",
        "    if (filtro_testado in [0, 3, 6] and filtro_final == 0) or \\\n",
        "       (filtro_testado in [1, 4, 7] and filtro_final == 1) or \\\n",
        "       (filtro_testado in [2, 5, 8] and filtro_final == 2):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def calcular_continuar1(row):\n",
        "    filtro_testado = 2 #row['filtro_testado_0']\n",
        "    filtro_final = row['filtro_final_x']\n",
        "\n",
        "    if (filtro_testado in [0, 3, 6] and filtro_final == 0) or \\\n",
        "       (filtro_testado in [1, 4, 7] and filtro_final == 1) or \\\n",
        "       (filtro_testado in [2, 5, 8] and filtro_final == 2):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "#calcular_continuar2=calcular_continuar3 ?\n",
        "def calcular_continuar2(row):\n",
        "    filtro_testado = row['filtro_testado_2']\n",
        "    filtro_final = row['filtro_final_y']\n",
        "\n",
        "    if (filtro_testado in [0, 1, 2] and filtro_final == 0) or \\\n",
        "       (filtro_testado in [3, 4, 5] and filtro_final == 1) or \\\n",
        "       (filtro_testado in [6, 7, 8] and filtro_final == 2):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def calcular_continuar3(row):\n",
        "    filtro_testado = row['filtro_testado_2']\n",
        "    filtro_final = row['filtro_final_y']\n",
        "\n",
        "    if (filtro_testado in [0, 1, 2] and filtro_final == 0) or \\\n",
        "       (filtro_testado in [3, 4, 5] and filtro_final == 1) or \\\n",
        "       (filtro_testado in [6, 7, 8] and filtro_final == 2):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "#teste\n",
        "def calcular_continuar4(row):\n",
        "  if row['filtro_final_y']==0:\n",
        "    return row['filtro_final_x']-1\n",
        "  else:\n",
        "    return 3*row['filtro_final_y']+row['filtro_final_x']-1"
      ],
      "metadata": {
        "id": "hNQ3kqa18BD9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treino modelo"
      ],
      "metadata": {
        "id": "GvC3mGx19pCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file_path='dataset_final.csv'\n",
        "file_path='sample_100pc.csv'\n",
        "chunk_size=100000"
      ],
      "metadata": {
        "id": "jV7hEGfb90dM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts = 0.3  # Tamanho do conjunto de teste\n",
        "rs = 16   # Semente aleatória\n",
        "#file_path = 'sample_23pc.csv'\n",
        "\n",
        "features0 = [\n",
        "    \"rd\", \"switchable_rate\", \"switchable_ctx_0\", \"switchable_ctx_1\", \"need_search\",\n",
        "    \"bsize\", \"partition\", \"mode\", \"uv_mode\", \"current_qindex\", \"default_interp_skip_flags\",\n",
        "    \"interp_filter_search_mask\", \"mi_row\", \"mi_col\", \"width\", \"height\", \"mb_to_bottom_edge\",\n",
        "    \"mb_to_left_edge\", \"mb_to_right_edge\", \"mb_to_top_edge\", \"rd_stats_luma_rate\",\n",
        "    \"rd_stats_luma_dist\", \"rd_stats_luma_sse\", \"rd_stats_luma_skip_txfm\", \"rd_stats_rate\",\n",
        "    \"rd_stats_dist\", \"rd_stats_sse\", \"rd_stats_skip_txfm\",\n",
        "    #'filtro_final_x'\n",
        "]\n",
        "colunas_a_remover=[\n",
        "    'need_search', 'uv_mode', 'default_interp_skip_flags', 'interp_filter_search_mask', 'filtro_testado_0',    'filtros_testado_filter_idx_0', 'filtro_testado_1', 'filtros_testado_filter_idx_1', 'filtros_testado_filter_idx_2',    'filtros_testado_filter_idx_3', 'filtro_testado_4', 'filtros_testado_filter_idx_4', 'filtros_testado_min_rd_4',    'filtros_testado_tmp_rd_4', 'tmp_rd_stats_rate_4', 'tmp_rd_stats_dist_4', 'tmp_rd_stats_sse_4', 'tmp_rd_stats_skip_txfm_4',    'filtro_testado_5', 'filtros_testado_filter_idx_5', 'filtros_testado_min_rd_5', 'filtros_testado_tmp_rd_5',    'tmp_rd_stats_rate_5', 'tmp_rd_stats_dist_5', 'tmp_rd_stats_sse_5', 'tmp_rd_stats_skip_txfm_5', 'filtro_testado_6',    'filtros_testado_filter_idx_6', 'filtros_testado_min_rd_6', 'filtros_testado_tmp_rd_6', 'tmp_rd_stats_rate_6',    'tmp_rd_stats_dist_6', 'tmp_rd_stats_sse_6', 'tmp_rd_stats_skip_txfm_6', 'filtro_testado_7', 'filtros_testado_filter_idx_7','filtros_testado_min_rd_7', 'filtros_testado_tmp_rd_7', 'tmp_rd_stats_rate_7', 'tmp_rd_stats_dist_7', 'tmp_rd_stats_sse_7',    'tmp_rd_stats_skip_txfm_7', 'filtro_testado_8', 'filtros_testado_filter_idx_8', 'filtros_testado_min_rd_8', 'filtros_testado_tmp_rd_8','tmp_rd_stats_rate_8', 'tmp_rd_stats_dist_8', 'tmp_rd_stats_sse_8', 'tmp_rd_stats_skip_txfm_8']\n",
        "\n",
        "features = [f for f in features0 if f not in colunas_a_remover]\n",
        "#notX = ['filtro_final_x', 'filtro_final_y','continuar','filtro_testado_2']"
      ],
      "metadata": {
        "id": "JlVuFaSm9_f2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REG_REG"
      ],
      "metadata": {
        "id": "t12bqjnr_0SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "    chunk['continuar'] = chunk.apply(calcular_continuarRR, axis=1)\n",
        "\n",
        "    X_chunk = chunk[features]\n",
        "    y_chunk = chunk[target]\n",
        "\n",
        "    X.append(X_chunk)\n",
        "    y.append(y_chunk)\n",
        "\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)\n"
      ],
      "metadata": {
        "id": "H10eh-IL9uC-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "dP6s2Jk2_zmu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT.fit(X_train, y_train)\n",
        "test_score = clfDT.score(X_test, y_test)\n",
        "y_pred = clfDT.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(clfDT, 'Reg_Reg.pkl')"
      ],
      "metadata": {
        "id": "D-TZFMNqAEAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625b8e3e-b316-4114-82c0-00471a509843"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.60      0.68    378858\n",
            "           1       0.90      0.95      0.93   1489706\n",
            "\n",
            "    accuracy                           0.88   1868564\n",
            "   macro avg       0.84      0.78      0.80   1868564\n",
            "weighted avg       0.88      0.88      0.88   1868564\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Reg_Reg.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REG_SMO"
      ],
      "metadata": {
        "id": "CVkydfCFGiC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] != 0]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarRM, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1b9mnz-GmuN",
        "outputId": "3005368e-0c33-4c0c-f83f-4862843ce2ba"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_100pc.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=80,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "YZJWw7MJH0ty"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Reg_Smo.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dujYfGlcH3Zp",
        "outputId": "7628c194-e035-406f-ee42-b13e704da5bc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 16s, sys: 189 ms, total: 1min 17s\n",
            "Wall time: 1min 17s\n",
            "Teste score para o modelo: 0.808839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.42      0.50    336103\n",
            "           1       0.84      0.92      0.88   1154270\n",
            "\n",
            "    accuracy                           0.81   1490373\n",
            "   macro avg       0.73      0.67      0.69   1490373\n",
            "weighted avg       0.79      0.81      0.80   1490373\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Reg_Smo.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REG_SHA"
      ],
      "metadata": {
        "id": "kVz5dQDKZStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] > 1]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarRH, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "uPDylgtRZ0fI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c7c553-a7b4-45a5-f905-97f386c88cc1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_100pc.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 7s, sys: 648 ms, total: 1min 8s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "99QNk33zZ5iz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Reg_Sha.pkl')"
      ],
      "metadata": {
        "id": "pYHAriMOZ-CY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3c3fdc-76e9-486c-e9f8-3b6fa701a181"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.2 s, sys: 57 ms, total: 48.3 s\n",
            "Wall time: 48.7 s\n",
            "Teste score para o modelo: 0.910670\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.67      0.74    155035\n",
            "           1       0.93      0.97      0.95    677707\n",
            "\n",
            "    accuracy                           0.91    832742\n",
            "   macro avg       0.87      0.82      0.84    832742\n",
            "weighted avg       0.91      0.91      0.91    832742\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Reg_Sha.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SMO_REG"
      ],
      "metadata": {
        "id": "YNT-90RuaO2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] != 0]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarMR, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "Ce6PKw2qdETM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553ea17f-1895-46e4-b170-aeab93230cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_100pc.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "eMjGFD43dLrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Smo_Reg.pkl')"
      ],
      "metadata": {
        "id": "m1N-SLzxdQS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SMO_SMO"
      ],
      "metadata": {
        "id": "V7-bj9-XaVQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] > 1]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarMM, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "BeGn2bHIm0zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "RcFT8VLXnFBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Smo_Smo.pkl')"
      ],
      "metadata": {
        "id": "Zr32FopDnF0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SMO_SHA"
      ],
      "metadata": {
        "id": "BtVAQMTXaZJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] > 2]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarMH, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "-nn-Ee-SnTW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Su_lvjrbnbVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Sha_Smo.pkl')"
      ],
      "metadata": {
        "id": "6FFh5tWHngI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHA_REG"
      ],
      "metadata": {
        "id": "KnJ1FupAasGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] > 1]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarHR, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "cPXyVcYWoENM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "c_yaR0pqoE-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Sha_Reg.pkl')"
      ],
      "metadata": {
        "id": "PMscp-tooEtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHA_SMO"
      ],
      "metadata": {
        "id": "hD5VI9VEawFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "print(file_path)\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "      # Calcular a coluna \"continuar\"\n",
        "      chunk = chunk[chunk['filtro_final_x'] + chunk['filtro_final_y'] > 2]\n",
        "\n",
        "      chunk['continuar'] = chunk.apply(calcular_continuarHM, axis=1)\n",
        "\n",
        "      # Separar os dados de entrada (X) e o target (y)\n",
        "      X_chunk = chunk[features]\n",
        "      y_chunk = chunk[target]\n",
        "\n",
        "      X.append(X_chunk)\n",
        "      y.append(y_chunk)\n",
        "\n",
        "# Concatenar todos os chunks em um único DataFrame\n",
        "X = pd.concat(X, ignore_index=True)\n",
        "y = pd.concat(y, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)"
      ],
      "metadata": {
        "id": "Igm82vWNoVYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfDT = DecisionTreeClassifier(\n",
        "    max_depth=16,\n",
        "    min_samples_split=60000,\n",
        "    max_leaf_nodes=60,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "LnoPEG0toVzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time modelo.fit(X_train, y_train)\n",
        "test_score = modelo.score(X_test, y_test)\n",
        "print(\"Teste score para o modelo: %f\" % test_score)\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(modelo, 'Sha_Smo.pkl')"
      ],
      "metadata": {
        "id": "ZjihTAIYoWLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}